# üìò HW4: –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Apache Spark —Å YARN –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å HDFS + Hive

## üìã –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Apache Spark –ø–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º YARN –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–º –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞–Ω–∏—è—Ö. –†–∞–±–æ—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç —á—Ç–µ–Ω–∏–µ, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Hive, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —á—Ç–µ–Ω–∏—è —á–µ—Ä–µ–∑ Hive CLI.

---

## üß∞ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Ubuntu 24.04
- Python 3.12.3
- Java 11.0.26
- –†–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –∫–ª–∞—Å—Ç–µ—Ä HDFS
- –†–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π Hive Metastore

---

## üîê –®–∞–≥ 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≥–ª–∞–≤–Ω–æ–º—É —É–∑–ª—É (Jump Node)

```bash
ssh -L 9870:127.0.0.1:9870 -L 8088:127.0.0.1:8088 -L 19888:127.0.0.1:19888 team@<JUMP-HOST-IP>
```

–ó–∞–º–µ–Ω–∏—Ç–µ `<JUMP-HOST-IP>` –Ω–∞ IP-–∞–¥—Ä–µ—Å –≤–∞—à–µ–π jump-–Ω–æ–¥—ã.

---

## ‚öôÔ∏è –®–∞–≥ 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:

```bash
sudo apt update
sudo apt install python3-venv python3-pip
```

### –ü–µ—Ä–µ—Ö–æ–¥ –≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è Hadoop:

```bash
sudo -i -u hadoop
```

### –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ Apache Spark:

```bash
wget https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
tar -xzvf spark-3.5.3-bin-hadoop3.tgz
```

---

## üåç –®–∞–≥ 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
export HADOOP_CONF_DIR="/home/hadoop/hadoop-3.4.0/etc/hadoop"
export HIVE_HOME="/home/apache-hive-4.0.1-bin"
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HIVE_AUX_JARS_PATH=$HIVE_HOME/lib/*
export PATH=$PATH:$HIVE_HOME/bin
export SPARK_LOCAL_IP=192.168.1.14 # –£–∫–∞–∂–∏—Ç–µ IP –≤–∞—à–µ–π jump-–Ω–æ–¥—ã
export SPARK_DIST_CLASSPATH="/home/hadoop/spark-3.5.3-bin-hadoop3/jars/*:/home/hadoop/hadoop-3.4.0/etc/hadoop:/home/hadoop/hadoop-3.4.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/common/*:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-3.4.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn/*:/home/hadoop/apache-hive-4.0.0-alpha-2-bin/*:/home/hadoop/apache-hive-4.0.0-alpha-2-bin/lib/*"
cd spark-3.5.3-bin-hadoop3/
export SPARK_HOME=`pwd`
export PYTHONPATH=$(ZIPS=("$SPARK_HOME"/python/lib/*.zip); IFS=:; echo "${ZIPS[*]}"):$PYTHONPATH
export PATH=$SPARK_HOME/bin:$PATH
```

---

## üß™ –®–∞–≥ 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Python –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
cd ~
python3 -m venv venv
source venv/bin/activate

pip install -U pip
pip install ipython
pip install onetl[files]
```

---

## üìÇ –®–∞–≥ 5. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ HDFS

```bash
hdfs dfs -mkdir -p /input
wget <–°–°–´–õ–ö–ê_–ù–ê_–î–ê–ù–ù–´–ï> -O for_spark.csv
hdfs dfs -put for_spark.csv /input
```

---

## üöÄ –®–∞–≥ 6. –ó–∞–ø—É—Å–∫ Spark-—Å–µ—Å—Å–∏–∏ —Å Hive –∏ YARN

```bash
ipython
```

### üì¶ –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ HDFS:

```python
from pyspark.sql import SparkSession, functions as F
from onetl.connection import SparkHDFS, Hive
from onetl.file import FileDFReader
from onetl.file.format import CSV
from onetl.db import DBWriter

spark = SparkSession.builder \
    .master("yarn") \
    .appName("spark-with-yarn") \
    .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
    .config("spark.hive.metastore.uris", "thrift://tmpl-jn:9083") \
    .enableHiveSupport() \
    .getOrCreate()

hdfs = SparkHDFS(host="tmpl-nn", port=9000, spark=spark, cluster="test")
reader = FileDFReader(connection=hdfs, format=CSV(delimiter=",", header=True), source_path="/input")
df = reader.run(["for_spark.csv"])
df.count() # –û–±—â–µ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫
df.rdd.getNumPartitions() # –ß–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π
```

---

## üîÅ –®–∞–≥ 7. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ

–î–æ–±–∞–≤–∏–º –∫–æ–ª–æ–Ω–∫—É `reg_year` –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è:

```python
df = df.withColumn("reg_year", F.col("registration date").substr(0, 4))
df.select("reg_year").distinct().show() # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
```

---

## üìù –®–∞–≥ 8. –ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤ Hive —Ç—Ä–µ–º—è —Å–ø–æ—Å–æ–±–∞–º–∏

### ‚úÖ –°–ø–æ—Å–æ–± 1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–ø–∏—Å—å —á–µ—Ä–µ–∑ Spark –±–µ–∑ —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è –ø–∞—Ä—Ç–∏—Ü–∏–π

–°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É `test.spark_auto`:

```python
from onetl.db import DBWriter
from onetl.connection import Hive

hive = Hive(spark=spark, cluster="test")
hive.check()  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è

writer = DBWriter(
    connection=hive,
    table="test.spark_auto",
    options={"if_exists": "replace_entire_table"},
)
writer.run(df)
```

---

### ‚úÖ –°–ø–æ—Å–æ–± 2. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–∞—Ä—Ç–∏—Ü–∏–π Spark –≤ –æ–¥–Ω—É (coalesce)

–°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É `test.spark_single_partition`:

```python
df_single_partition = df.coalesce(1)

writer = DBWriter(
    connection=hive,
    table="test.spark_single_partition",
    options={"if_exists": "replace_entire_table"},
)
writer.run(df_single_partition)
```

---

### ‚úÖ –°–ø–æ—Å–æ–± 3. –ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Hive –ø–æ —Å—Ç–æ–ª–±—Ü—É `reg_year`

–°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É `test.hive_partitioned`:

```python
writer = DBWriter(
    connection=hive,
    table="test.hive_partitioned",
    options={
        "if_exists": "replace_entire_table",
        "partitionBy": "reg_year",
    },
)
writer.run(df)
```

---

## üîç –®–∞–≥ 9. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Hive CLI

```sql
hive
> USE test;
> SHOW TABLES;
> SELECT COUNT(*) FROM spark_auto;
> SELECT COUNT(*) FROM spark_single_partition;
> SELECT reg_year, COUNT(*) FROM hive_partitioned GROUP BY reg_year;
```

---

## ‚õî –®–∞–≥ 10. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏

```python
spark.stop()
```

---
