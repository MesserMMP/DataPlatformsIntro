# üìò HW4: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Apache Spark –ø–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º YARN


## üõ†Ô∏è –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–∫–∏

- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å —É –≤–∞—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –≤–µ—Ä—Å–∏—è Java, —Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è —Å–æ `Spark`
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–±—ä—è–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è `HADOOP_HOME` –∏ –æ–Ω–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ `Hadoop`
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ `PATH` –¥–æ–±–∞–≤–ª–µ–Ω—ã –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–µ —Ñ–∞–π–ª—ã –¥–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–∞ `Hadoop`


## üîê –®–∞–≥ 1: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∫–ª–∞—Å—Ç–µ—Ä—É

–ü–æ–¥–∫–ª—é—á–∏—Ç–µ—Å—å –∫ –≥–ª–∞–≤–Ω–æ–º—É —É–∑–ª—É —Å –≤–∞—à–µ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞, –ø—Ä–æ–∫–∏–Ω—É–≤ –ø–æ—Ä—Ç—ã:

```bash
ssh -L 9870:127.0.0.1:9870 -L 8088:127.0.0.1:8088 -L 19888:127.0.0.1:19888 team@176.109.91.5
```

---


–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `python-venv` –∏ `python-pip` –∏ `IPython`:

```bash
sudo apt install python3-venv
sudo apt install python3-pip
```

## üë§ –®–∞–≥ 2: –ü–µ—Ä–µ—Ö–æ–¥ –≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è `hadoop` –∏ –¥–∞–ª–µ–µ

```bash
sudo -i -u hadoop
```

–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä—É–π—Ç–µ –¥–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤ `Spark`:
```bash
wget https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
tar -xzvf spark-3.5.3-bin-hadoop3.tgz
```
## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ä–µ–¥—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å–æ `Spark`

–û–±—ä—è–≤–∏—Ç–µ —Ä—è–¥ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, –¥–ª—è `SPARK_LOCAL_IP` —É–∫–∞–∂–∏—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–π ip-–∞–¥—Ä–µ—Å jump-–Ω–æ–¥—ã: 

```bash
export HADOOP_CONF_DIR="/home/hadoop/hadoop-3.4.0/etc/hadoop"
export HIVE_HOME="/home/apache-hive-4.0.1-bin"
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HIVE_AUX_JARS_PATH=$HIVE_HOME/lib/*
export PATH=$PATH:$HIVE_HOME/bin
export SPARK_LOCAL_IP=192.168.1.14
export SPARK_DIST_CLASSPATH="/home/hadoop/spark-3.5.3-bin-hadoop3/jars/*:/home/hadoop/hadoop-3.4.0/etc/hadoop:/home/hadoop/hadoop-3.4.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/common/*:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-3.4.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn/*:/home/hadoop/apache-hive-4.0.0-alpha-2-bin/*:/home/hadoop/apache-hive-4.0.0-alpha-2-bin/lib/*"
cd spark-3.5.3-bin-hadoop3/
export SPARK_HOME=`pwd`
export PYTHONPATH=$(ZIPS=("$SPARK_HOME"/python/lib/*.zip); IFS=:; echo "${ZIPS[*]}"):$PYTHONPATH
export PATH=$SPARK_HOME/bin:$PATH
```

–í–µ—Ä–Ω–∏—Ç–µ—Å—å –≤ –¥–æ–º–∞—à–Ω—é—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ —Å–æ–∑–¥–∞–¥–∏–º –Ω–æ–≤–æ–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ—Ä–∫—Ä—É–∂–µ–Ω–∏–µ:
```bash
cd ~
python3 -m venv venv
source venv/bin/activate
```

–û–±–Ω–æ–≤–∏—Ç–µ `pip`, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `IPython` –∏ `onetl[files]`:
```bash
pip install -U pip
pip install ipython
pip install onetl[files]
```

–í —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ `hadoop` —Å–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É `input` –∏ –ø–æ–ª–æ–∂–∏—Ç–µ —Ç—É–¥–∞ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å–æ `Spark` –≤ —Ñ–æ—Ä–º–∞—Ç–µ `.csv`:
```bash
hdfs dfs -mkdir -p /input
# –í–º–µ—Å—Ç–æ ... —É–∫–∞–∂–∏—Ç–µ —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
wget ... 
hdfs dfs -put for_spark.csv /input
```

–¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Å—Å–∏—é `Spark`, —á—Ç–æ–±—ã –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç `.csv` —Ñ–∞–π–ª –≤ —Ç–∞–±–ª–∏—Ü—É, –æ–ø–∏—Å–∞–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é `hive`:
```bash
ipython
```

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from onetl.connection import SparkHDFS
from onetl.connection import Hive
from onetl.file import FileDFReader
from onetl.file.format import CSV
from onetl.db import DBWriter
spark = SparkSession.builder \
    .master("yarn") \
    .appName("spark-with-yarn") \
    .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
    .config("spark.hive.metastore.uris", "thrift://tmpl-jn:9083") \
    .enableHiveSupport() \
    .getOrCreate()
hdfs = SparkHDFS(host="tmpl-nn", port=9000, spark=spark, cluster="test")
hdfs.check()
reader = FileDFReader(connection=hdfs, format=CSV(delimiter=",", header=True), source_path="/input")
df = reader.run(["for_spark.csv"]) # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ, —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –¥–∞—Ç–∞ —Ñ—Ä–µ–π–º
df.count() # –û–±—â–µ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫
df.rdd.getNumPartitions() # –ß–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π
```
–°–æ–∑–¥–∞–π—Ç–µ —Å—Ç–æ–ª–±–µ—Ü –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ –¥–∞—Ç–∞ —Ñ—Ä–µ–π–º
```python
dt = df.select("registrstion date")
dt.show()
df = df.withColumn("reg_year", F.col("registrstion date").substr(0, 4))
dt = df.select("reg_year")
dt.show()
```

–ó–∞–ø–∏—à–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—É:
```python
hive = Hive(spark=spark, cluster="test")
hive.check() # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
writer = DBWriter(connection=hive, table="test.spark_parts", options={"if_exists": "replace_entire_table"})
writer.run(df)
# writer.run(df.coalesce(1)) # –°–∫–ª–µ–∏–≤–∞–µ—Ç –≤—Å–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏ Spark –≤ 1
```

–ü—Ä–∏–º–µ—Ä –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ `Hive`
```python
hive = Hive(spark=spark, cluster="test")
hive.check() # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
writer = DBWriter(connection=hive, table="test.hive_parts", options={"if_exists": "replace_entire_table", "partitionBy": "reg_year"}) # reg_year - —Å—Ç–æ–ª–±–µ—Ü, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
writer.run(df)
```

–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Å—Å–∏–∏
```python
spark.stop()
quit()
```
