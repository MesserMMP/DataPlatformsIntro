# üìò HW4: –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Apache Spark —Å YARN –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å HDFS + Hive

## üìã –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Apache Spark –ø–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º YARN –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–º –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞–Ω–∏—è—Ö. –†–∞–±–æ—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç —á—Ç–µ–Ω–∏–µ, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Hive, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —á—Ç–µ–Ω–∏—è —á–µ—Ä–µ–∑ Hive CLI.

---

## üß∞ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Ubuntu 24.04
- Python 3.12.3
- Java 11.0.26
- –†–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –∫–ª–∞—Å—Ç–µ—Ä HDFS
- –†–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π Hive Metastore

---

## üîê –®–∞–≥ 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≥–ª–∞–≤–Ω–æ–º—É —É–∑–ª—É (Jump Node)

```bash
ssh -L 9870:127.0.0.1:9870 -L 8088:127.0.0.1:8088 -L 19888:127.0.0.1:19888 team@<JUMP-HOST-IP>
```

–ó–∞–º–µ–Ω–∏—Ç–µ `<JUMP-HOST-IP>` –Ω–∞ IP-–∞–¥—Ä–µ—Å –≤–∞—à–µ–π jump-–Ω–æ–¥—ã.

---

## ‚öôÔ∏è –®–∞–≥ 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:

```bash
sudo apt update
sudo apt install python3-venv python3-pip
```

### –ü–µ—Ä–µ—Ö–æ–¥ –≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è Hadoop:

```bash
sudo -i -u hadoop
```

### –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ Apache Spark:

```bash
wget https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
tar -xzvf spark-3.5.3-bin-hadoop3.tgz
```

---

## üåç –®–∞–≥ 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
export HADOOP_CONF_DIR="/home/hadoop/hadoop-3.4.0/etc/hadoop"
export HIVE_HOME="/home/hadoop/apache-hive-4.0.0-alpha-2-bin"
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HIVE_AUX_JARS_PATH=$HIVE_HOME/lib/*
export PATH=$PATH:$HIVE_HOME/bin
export SPARK_LOCAL_IP=192.168.1.14 # –£–∫–∞–∂–∏—Ç–µ IP –≤–∞—à–µ–π jump-–Ω–æ–¥—ã
export SPARK_DIST_CLASSPATH="/home/hadoop/spark-3.5.3-bin-hadoop3/jars/*:/home/hadoop/hadoop-3.4.0/etc/hadoop:/home/hadoop/hadoop-3.4.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/common/*:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-3.4.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn/*:/home/hadoop/apache-hive-4.0.0-alpha-2-bin/*:/home/hadoop/apache-hive-4.0.0-alpha-2-bin/lib/*"

cd spark-3.5.3-bin-hadoop3/
export SPARK_HOME=`pwd`
export PYTHONPATH=$(ZIPS=("$SPARK_HOME"/python/lib/*.zip); IFS=:; echo "${ZIPS[*]}"):$PYTHONPATH
export PATH=$SPARK_HOME/bin:$PATH
```

---

## üß™ –®–∞–≥ 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Python –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
cd ~
python3 -m venv venv
source venv/bin/activate

pip install -U pip
pip install ipython
pip install onetl[files]
```

---

## üìÇ –®–∞–≥ 5. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ HDFS

```bash
hdfs dfs -mkdir -p /input
wget https://raw.githubusercontent.com/MesserMMP/Datasets/main/Electric_Vehicle_Population_Data.csv -O electric_vehicles.csv
hdfs dfs -put electric_vehicles.csv /input
```

---

## üöÄ –®–∞–≥ 6. –ó–∞–ø—É—Å–∫ Spark-—Å–µ—Å—Å–∏–∏ —Å Hive –∏ YARN

–ü–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ –∑–∞–ø—É—Å–∫–∞—Ç—å Spark-—Å–µ—Å—Å–∏—é —Å Hive, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å Hive Metastore –≤—Ä—É—á–Ω—É—é –≤ —Ñ–æ–Ω–µ (–µ—Å–ª–∏ –æ–Ω –µ—â—ë –Ω–µ –∑–∞–ø—É—â–µ–Ω):

```bash
hive --hiveconf hive.server2.enable.doAs=false \
     --hiveconf hive.security.authorization.enabled=false \
     --service metastore 1>> /tmp/metastore.log 2>> /tmp/metastore.log &
```

–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –æ–±–æ–ª–æ—á–∫—É Python:
```bash
ipython
```

### üì¶ –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ HDFS:

```python
from pyspark.sql import SparkSession, functions as F
from onetl.connection import SparkHDFS, Hive
from onetl.file import FileDFReader
from onetl.file.format import CSV
from onetl.db import DBWriter

spark = SparkSession.builder \
    .master("yarn") \
    .appName("spark-with-yarn") \
    .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
    .config("spark.hive.metastore.uris", "thrift://tmpl-jn:9083") \
    .enableHiveSupport() \
    .getOrCreate()

hdfs = SparkHDFS(host="tmpl-nn", port=9000, spark=spark, cluster="test")
reader = FileDFReader(connection=hdfs, format=CSV(delimiter=",", header=True), source_path="/input")
df = reader.run(["electric_vehicles.csv"])
df.count() # –û–±—â–µ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫
df.rdd.getNumPartitions() # –ß–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π (2)
dt = df.select("Model Year") # –°—Ç–æ–ª–±–µ—Ü –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è Hive
dt.show()
```
*–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã –∏–∑ –≤–µ–± –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞:*

![Spark-Jobs](./screenshots/spark-jobs.png)

---

## üîÅ –®–∞–≥ 7. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π (–∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤)

–ü—Ä–∏–≤–µ–¥–∏—Ç–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∫ –Ω—É–∂–Ω—ã–º —Ç–∏–ø–∞–º, –∞ –∑–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∞–≥—Ä–µ–≥–∞—Ü–∏—é:

```python
from pyspark.sql.types import IntegerType, DoubleType

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤
df_transformed = df \
    .withColumn("Model Year", F.col("Model Year").cast(IntegerType())) \
    .withColumn("Electric Range", F.col("Electric Range").cast(IntegerType())) \
    .withColumn("Base MSRP", F.col("Base MSRP").cast(DoubleType()))

# –ü—Ä–∏–º–µ—Ä –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: —Å—Ä–µ–¥–Ω–∏–π –∑–∞–ø–∞—Å —Ö–æ–¥–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –≥–æ–¥—É
df_agg = df_transformed.groupBy("Model Year").agg(
    F.count("*").alias("vehicle_count"),
    F.avg("Electric Range").alias("avg_range"),
    F.avg("Base MSRP").alias("avg_msrp")
)

df_agg.orderBy("Model Year").show()
```

–≠—Ç–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–æ–∑–≤–æ–ª—è—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Spark –∫–∞–∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ EV-–¥–∞—Ç–∞.

---

## üìù –®–∞–≥ 8. –ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤ Hive —Ç—Ä–µ–º—è —Å–ø–æ—Å–æ–±–∞–º–∏

### ‚úÖ –°–ø–æ—Å–æ–± 1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–ø–∏—Å—å —á–µ—Ä–µ–∑ Spark –±–µ–∑ —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è –ø–∞—Ä—Ç–∏—Ü–∏–π

–°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É `test.spark_auto`:

```python
hive = Hive(spark=spark, cluster="test")
hive.check()  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è

writer = DBWriter(
    connection=hive,
    table="test.spark_auto",
    options={"if_exists": "replace_entire_table"},
)
writer.run(df)
```

*–ö–∞–∫ –≤–∏–¥–Ω–æ –∏–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –±—ã–ª–æ —Å–æ–∑–¥–∞–Ω–æ 2 —Ç–∞–±–ª–∏—Ü—ã –ø–æ —á–∏—Å–ª—É –ø–∞—Ä—Ç–∏—Ü–∏–π:*

![Spark_Auto_Partition](./screenshots/spark_auto_partition.png)

---

### ‚úÖ –°–ø–æ—Å–æ–± 2. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–∞—Ä—Ç–∏—Ü–∏–π Spark –≤ –æ–¥–Ω—É (coalesce)

–°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É `test.spark_single_partition`:

```python
df_single_partition = df.coalesce(1)

writer = DBWriter(
    connection=hive,
    table="test.spark_single_partition",
    options={"if_exists": "replace_entire_table"},
)
writer.run(df_single_partition)
```

*–ö–∞–∫ –≤–∏–¥–Ω–æ –∏–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Ç—É—Ç —É–∂–µ —Ç–æ–ª—å–∫–æ 1 —Ç–∞–±–ª–∏—Ü–∞:*

![Spark_Single_Partition](./screenshots/spark_single_partition.png)

---

### ‚úÖ –°–ø–æ—Å–æ–± 3. –ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Hive –ø–æ —Å—Ç–æ–ª–±—Ü—É `Model Year`

–°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É `test.hive_partitioned`:

```python
writer = DBWriter(
    connection=hive,
    table="test.hive_partitioned",
    options={
        "if_exists": "replace_entire_table",
        "partitionBy": "Model Year",
    },
)
writer.run(df)
```

*–ö–∞–∫ –≤–∏–¥–Ω–æ –∏–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Ç—É—Ç —É–∂–µ —Ç–æ–ª—å–∫–æ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≥–æ–¥–∞ `Model Year`:*

![Hive_Partition](./screenshots/hive_partition.png)

*–í—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã:*

![All_tables](./screenshots/all_tables.png)


---

## üîç –®–∞–≥ 9. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Hive CLI


```bash
beeline -u jdbc:hive2://tmpl-jn:5432 -n scott -p tiger
```

```sql
USE test;
SHOW TABLES;
SELECT * FROM spark_auto LIMIT 5;
SELECT * FROM spark_single_partition LIMIT 5;
SELECT * FROM hive_partitioned LIMIT 5;
```

---

## ‚õî –®–∞–≥ 10. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏

```python
spark.stop()
```

---
