# üöÄ HW5: Data Processing Flow with Prefect + Apache Spark on YARN

## üîê –®–∞–≥ 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≥–ª–∞–≤–Ω–æ–º—É —É–∑–ª—É (Jump Node) –∏ –≤—Ö–æ–¥ –≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è `hadoop`

```bash
ssh -L 9870:127.0.0.1:9870 -L 8088:127.0.0.1:8088 -L 19888:127.0.0.1:19888 team@<JUMP-HOST-IP>
sudo -i -u hadoop
```
–ó–∞–º–µ–Ω–∏—Ç–µ `<JUMP-HOST-IP>` –Ω–∞ IP-–∞–¥—Ä–µ—Å –≤–∞—à–µ–π jump-–Ω–æ–¥—ã.

## –û—Å—Ç–∞–ª—å–Ω—ã–µ —à–∞–≥–∏

## üß™ –®–∞–≥ 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Python –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
cd ~
python3 -m venv venv
source venv/bin/activate
pip install prefect
```

–°–æ–∑–¥–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –¥–ª—è `prefect` - `etl_flow.py`:
```bash
vim etl_flow.py
```
`etl_flow.py`:
```python
from pyspark.sql import SparkSession, functions as F
from onetl.connection import SparkHDFS, Hive
from onetl.file import FileDFReader
from onetl.file.format import CSV
from onetl.db import DBWriter
from prefect import flow, task

@task
def get_spark():
    spark = SparkSession.builder \
        .master("yarn") \
        .appName("spark-with-yarn") \
        .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
        .config("spark.hive.metastore.uris", "thrift://tmpl-jn:9083") \
        .enableHiveSupport() \
        .getOrCreate()
    return spark


@task
def stop_spark(spark):
    spark.stop()


@task
def extract(spark):
    hdfs = SparkHDFS(host="tmpl-nn", port=9000, spark=spark, cluster="test")
    reader = FileDFReader(connection=hdfs, format=CSV(delimiter=",", header=True), source_path="/input")
    df = reader.run(["electric_vehicles.csv"])
    return df


@task
def transform(df):
    df_cleaned = df.withColumn("Model Year", F.col("Model Year").cast("int")) # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç–∏–ø–æ–≤
    df_agg = df_cleaned.groupBy("Make").count().orderBy("count", ascending=False) # –ê–≥—Ä–µ–≥–∞—Ü–∏—è
    return df_agg


@task
def load(spark, df):
    hive = Hive(spark=spark, cluster="test")
    writer = DBWriter(connection=hive, table="test.ev_aggregated_counts",
                      options={"if_exists": "replace_entire_table"})
    writer.run(df)


@flow
def process_data():
    spark = get_spark()
    df = extract(spark)
    df = transform(df)
    load(spark, df)
    stop_spark(spark)


if __name__ == "__main__":
    process_data()
```
 –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç
```bash
python etl_flow.py
```

*–ü–æ—è–≤–∏–ª–∞—Å—å –Ω–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ ev_aggregated_counts:*

![All_tables](./screenshots/all_tables.png)

## üîç –®–∞–≥ 9. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Hive CLI


```bash
beeline -u jdbc:hive2://tmpl-jn:5432 -n scott -p tiger
```

–°–¥–µ–ª–∞–π—Ç–µ `sql` –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏:
```sql
USE test;
SHOW TABLES;
SELECT * FROM ev_aggregated_counts LIMIT 10;
```

*–¢–∞–±–ª–∏—Ü–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:*

![sql_results](./screenshots/sql_results.png)

---

