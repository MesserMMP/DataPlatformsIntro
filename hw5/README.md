# HW5: Data Processing Flow with Prefect + Apache Spark on YARN

## Описание

Это решение для задания №5 по курсу **"Введение в платформы данных"**.  
Задание включает реализацию потока обработки данных с использованием:

- [Prefect](https://docs.prefect.io/) — библиотека для создания ETL/ELT-пайплайнов;
- Apache Spark на YARN — для масштабируемой обработки данных;
- Hive — для хранения результатов.

## Структура проекта
```
.
├── scripts/
|  ├── etl_flow.py         # Реализация Prefect-потока (extract → transform → load)
|  ├── run_etl.sh          # Скрипт автоматического запуска потока
├── instructions.md     # Подробная инструкция по запуску проекта
├── README.md           # Краткое описание проекта
└── screenshots/        # Скриншоты с результатами
```

## Используемые технологии

- Apache Spark (режим YARN)
- Prefect 2.x
- ONETL (для подключения к HDFS и Hive)
- Hive (внешний metastore)
- Python 3.12

## Что делает поток?

Поток `process_data` реализует полный ETL-процесс:

1. **`get_spark()`**  
   Инициализирует `SparkSession` с поддержкой Hive и подключением к YARN.

2. **`extract(spark)`**  
   Подключается к HDFS и читает CSV-файл `electric_vehicles.csv` из папки `/input/` с помощью ONETL.

3. **`transform(df)`**  
   Выполняет трансформации:
   - Преобразует столбец `Model Year` к типу `int`.
   - Группирует данные по бренду (`Make`) и считает количество автомобилей.
   - Сортирует результат по убыванию количества.

4. **`load(spark, df)`**  
   Сохраняет результат в таблицу Hive `test.ev_aggregated_counts`, полностью перезаписывая её при каждом запуске.

5. **`stop_spark(spark)`**  
   Завершает `SparkSession` после выполнения всех шагов.

## Как запустить

1. Подключитесь к главному узлу (см. `instructions.md`).
2. Запустите поток:

```bash
./run_etl.sh
```
3. Проверьте результаты в Hive CLI или через веб-интерфейс.

## Результат

- В Hive создана таблица ev_aggregated_counts с агрегированными данными.
- Поток реализован в виде Python-скрипта с использованием Prefect.
- Подключение к HDFS и Hive выполнено через ONETL.
- Данные обработаны с помощью Spark на YARN.
- Запуск автоматизирован через run_etl.sh.
